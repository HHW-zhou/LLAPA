{
  "architectures": [
    "LlapaForConditionalGeneration"
  ],
  "ignore_index": -100,
  "model_type": "llapa",
  "projector_hidden_act": "gelu",
  "protein_config": {
    "_name_or_path": "##################################### PATH to esm2-3b #################################################",
    "architectures": [
      "EsmModel"
    ],
    "attention_probs_dropout_prob": 0.0,
    "classifier_dropout": null,
    "emb_layer_norm_before": false,
    "esmfold_config": null,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.0,
    "hidden_size": 2560,
    "intermediate_size": 10240,
    "is_folding_model": false,
    "layer_norm_eps": 1e-05,
    "mask_token_id": 32,
    "max_position_embeddings": 1026,
    "model_type": "esm",
    "num_attention_heads": 40,
    "num_hidden_layers": 36,
    "pad_token_id": 1,
    "position_embedding_type": "rotary",
    "token_dropout": true,
    "torch_dtype": "bfloat16",
    "use_cache": true,
    "vocab_list": null,
    "vocab_size": 33
  },
  "protein_token_index": 32000,
  "text_config": {
    "_name_or_path": "##################################### PATH to Llama3.2-3B-Instruct-HF #################################################",   
    "architectures": [
      "LlamaForCausalLM"
    ],
    "max_length": 4096,
    "max_position_embeddings": 4096,
    "model_type": "llama",
    "pad_token_id": 0,
    "torch_dtype": "bfloat16",
    "use_cache": false,
    "vocab_size": 32001,
    "hidden_size":3072
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.40.2",
  "protein_feature_select_strategy": "local",
  "p_mode": "p",
  "ROOT_DIR": "##################################### YOUR WORK DIR #################################################",
  "DATA_DIR": "##################################### YOUR DATASET DIR #################################################",
  "backbone": "llama3"
}